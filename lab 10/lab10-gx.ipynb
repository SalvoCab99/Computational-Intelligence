{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright **`(c)`** 2023 Giovanni Squillero `<giovanni.squillero@polito.it>`  \n",
    "[`https://github.com/squillero/computational-intelligence`](https://github.com/squillero/computational-intelligence)  \n",
    "Free for personal or classroom use; see [`LICENSE.md`](https://github.com/squillero/computational-intelligence/blob/master/LICENSE.md) for details.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "from collections import namedtuple, defaultdict\n",
    "from random import choice\n",
    "import random\n",
    "from copy import deepcopy\n",
    "\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "State = namedtuple('State', ['x', 'o'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAGIC = [2, 7, 6, 9, 5, 1, 4, 3, 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_board(pos):\n",
    "    \"\"\"Nicely prints the board\"\"\"\n",
    "    for r in range(3):\n",
    "        for c in range(3):\n",
    "            i = r * 3 + c\n",
    "            if MAGIC[i] in pos.x:\n",
    "                print('X', end='')\n",
    "            elif MAGIC[i] in pos.o:\n",
    "                print('O', end='')\n",
    "            else:\n",
    "                print('.', end='')\n",
    "        print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def win(elements):\n",
    "    \"\"\"Checks is elements is winning\"\"\"\n",
    "    return any(sum(c) == 15 for c in combinations(elements, 3))\n",
    "\n",
    "def state_value(pos: State, player):\n",
    "    \"\"\"Evaluate state: +1 first player wins\"\"\"\n",
    "    if win(pos.x):\n",
    "        return 1 if player == 0 else -1\n",
    "    elif win(pos.o):\n",
    "        return 1 if player == 1 else -1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def get_dict(value_dictionary , state, action):         #this function return the value of making that specific action(action) in that situation(state)\n",
    "    state_key = (tuple(state.x), tuple(state.o))\n",
    "    return value_dictionary [(state_key, action)]\n",
    "\n",
    "def move_p(value_dictionary , state, available, epsilon=0.1):   #this function is used to do the action\n",
    "    if random.random() < epsilon:                                #it will do a random action if a random number is lower then epsilon\n",
    "        return random.choice(available)\n",
    "    else:\n",
    "        dict_values = [get_dict(value_dictionary, state, action) for action in available]    #if it is higher it will do the move with the highest value in the dictionary\n",
    "        best_value = max(dict_values)\n",
    "        best_moves = [i for i in range(len(available)) if dict_values[i] == best_value]\n",
    "        index = random.choice(best_moves)\n",
    "        return available[index]\n",
    "\n",
    "def update_dict(value_dictionary, state, action, reward, next_state, available, alpha=0.1, discount_factor=0.9):        #used to update dictionary\n",
    "    state_key = (tuple(state.x), tuple(state.o))\n",
    "    next_dict_values = [get_dict(value_dictionary, next_state, next_action) for next_action in available]\n",
    "    max_next_value = max(next_dict_values, default=0.0)\n",
    "    value_dictionary[(state_key, action)] = (1 - alpha) * value_dictionary[(state_key, action)] + alpha * (reward + discount_factor * max_next_value)\n",
    "\n",
    "\n",
    "def train(num_episodes, alpha, epsilon, disc_factor, player):       #training function\n",
    "    value_dictionary = defaultdict(float)\n",
    "    for _ in tqdm(range(num_episodes)):\n",
    "        state = State(set(), set())\n",
    "        available = list(range(1, 9 + 1))\n",
    "        player_turn = 0 \n",
    "\n",
    "        while available and not win(state):\n",
    "            if player_turn == player:\n",
    "                action = move_p(value_dictionary, state, available, epsilon)    #player action\n",
    "            else:\n",
    "                action = random.choice(available)                               #random action\n",
    "\n",
    "            previous_state = deepcopy(state)\n",
    "\n",
    "            if player_turn == 0:\n",
    "                state.x.add(action)\n",
    "            else:\n",
    "                state.o.add(action)\n",
    "\n",
    "            available.remove(action)\n",
    "\n",
    "            reward = state_value(state, player)\n",
    "            update_dict(value_dictionary, previous_state, action, reward, state, available, alpha, disc_factor)\n",
    "\n",
    "            # Switching the player turn\n",
    "            player_turn = 1 - player_turn\n",
    "    return value_dictionary\n",
    "\n",
    "def game(value_dictionary, player):  # Gaming section\n",
    "    trajectory = list()\n",
    "    state = State(set(), set())\n",
    "    available = list(range(1, 9 + 1))\n",
    "    if player == 0:\n",
    "        while available:\n",
    "            x = move_p(value_dictionary, state, available)\n",
    "            state.x.add(x)\n",
    "            trajectory.append(deepcopy(state))\n",
    "            available.remove(x)\n",
    "            if win(state.x) or not available:\n",
    "                break\n",
    "\n",
    "            o = random.choice(available)\n",
    "            state.o.add(o)\n",
    "            trajectory.append(deepcopy(state))\n",
    "            available.remove(o)\n",
    "            if win(state.o) or not available:\n",
    "                break\n",
    "    elif player == 1:\n",
    "        while available:\n",
    "            x = random.choice(available)\n",
    "            state.x.add(x)\n",
    "            trajectory.append(deepcopy(state))\n",
    "            available.remove(x)\n",
    "            if win(state.x) or not available:\n",
    "                break\n",
    "\n",
    "            o = move_p(value_dictionary, state, available)\n",
    "            state.o.add(o)\n",
    "            trajectory.append(deepcopy(state))\n",
    "            available.remove(o)\n",
    "            if win(state.o) or not available:\n",
    "                break\n",
    "    return trajectory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing section\n",
    "in this section you can modify NUM_games, NUM_EPISODES and player(0 or 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08ca156445d54242a539f1488e8b6bc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for 10_000 games:\n",
      "Win rate of our player(9149): 91.49%\n",
      "Win rate of random player(618): 6.18%\n",
      "Draw rate(233): 2.33%\n"
     ]
    }
   ],
   "source": [
    "NUM_GAMES = 10_000              #used for testing\n",
    "NUM_EPISODES = 100_000          #used for training\n",
    "player = 0  # our player that can be 0 or 1. 0 means \"x\" and 1 means \"o\"\n",
    "#alpha, epsilon and discount factor can be modified\n",
    "value_dictionary = train(num_episodes=NUM_EPISODES, alpha=0.5, epsilon=0.1, disc_factor=0.9, player=player) #training, fills the value_dictionary\n",
    "\n",
    "win_player = 0\n",
    "win_random = 0\n",
    "for i in range(NUM_GAMES):\n",
    "    trajectory = game(value_dictionary, player)\n",
    "    val_finished_game = state_value(trajectory[-1], player)\n",
    "    if val_finished_game == 1:\n",
    "        win_player += 1\n",
    "    elif val_finished_game == -1:\n",
    "        win_random += 1\n",
    "\n",
    "win_player_rate = (win_player / NUM_GAMES) \n",
    "win_random_rate = (win_random / NUM_GAMES) \n",
    "draw_rate = ((NUM_GAMES-win_random-win_player) / NUM_GAMES) \n",
    "\n",
    "print(\"Results for 10_000 games:\")\n",
    "print(f\"Win rate of our player({win_player}): {win_player_rate:.2%}\")\n",
    "print(f\"Win rate of random player({win_random}): {win_random_rate:.2%}\")\n",
    "print(f\"Draw rate({(NUM_GAMES-win_random-win_player)}): {draw_rate:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sorted(value_dictionary.items(), key=lambda e: e[1], reverse=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ci-P-7LqQ3C-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
